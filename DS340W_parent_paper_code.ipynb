{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "_kndg0t-DFqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BNjG6ClLBsNb"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node1_start, node1_end = (0, 218)\n",
        "node2_start, node2_end = (218, 13557)\n",
        "node3_start, node3_end = (13557, 14266)\n",
        "node4_start, node4_end = (14266, 67117)\n",
        "\n",
        "node_trackers = []\n",
        "node_trackers.append((0, 218))\n",
        "node_trackers.append((218, 13557))\n",
        "node_trackers.append((13557, 14266))\n",
        "node_trackers.append((14266, 67117))\n"
      ],
      "metadata": {
        "id": "vCtx3c1TCfci"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_type(node_no):\n",
        "        if node_no >= node1_start and node_no < node1_end:\n",
        "            return 0\n",
        "        elif node_no >= node2_start and node_no < node2_end:\n",
        "            return 1\n",
        "        elif node_no >= node3_start and node_no < node3_end:\n",
        "            return 2\n",
        "        else:\n",
        "            return 3"
      ],
      "metadata": {
        "id": "3Fz1GB6KCh9O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataUtils:\n",
        "    def __init__(self, graph_file, is_all=False, node_negative_distribution_temp = None, test_indices = None):\n",
        "        self.test_indices = test_indices\n",
        "\n",
        "        with np.load(graph_file, allow_pickle=True) as loader:\n",
        "            loader = dict(loader)\n",
        "            self.A = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
        "                               loader['adj_indptr']), shape=loader['adj_shape'])\n",
        "            \n",
        "            if loader['attr_data1'].all() != None:\n",
        "                print(\"Attributes Found.\")\n",
        "\n",
        "                self.X1 = sp.csr_matrix((loader['attr_data1'], loader['attr_indices1'],\n",
        "                                loader['attr_indptr1']), shape=loader['attr_shape1'])\n",
        "                self.X2 = sp.csr_matrix((loader['attr_data2'], loader['attr_indices2'],\n",
        "                                loader['attr_indptr2']), shape=loader['attr_shape2'])\n",
        "                self.X3 = sp.csr_matrix((loader['attr_data3'], loader['attr_indices3'],\n",
        "                                loader['attr_indptr3']), shape=loader['attr_shape3'])\n",
        "                self.X4 = sp.csr_matrix((loader['attr_data4'], loader['attr_indices4'],\n",
        "                                loader['attr_indptr4']), shape=loader['attr_shape4'])\n",
        "\n",
        "            \"\"\" self.node_type1 = loader['node_type1']\n",
        "            self.node_type2 = loader['node_type2'] \"\"\"\n",
        "\n",
        "            if 'labels' in loader.keys():\n",
        "                self.labels = loader['labels']\n",
        "            else:\n",
        "                self.labels = None\n",
        "\n",
        "            if not is_all and 'val_edges' in loader.keys():\n",
        "                raise Exception(\"val not included yet\")\n",
        "                \"\"\" self.val_edges = loader['val_edges']\n",
        "                self.val_ground_truth = loader['val_ground_truth']\n",
        "                self.test_edges = loader['test_edges']\n",
        "                self.test_ground_truth = loader['test_ground_truth'] \"\"\"\n",
        "\n",
        "            self.g = nx.from_scipy_sparse_matrix(self.A, create_using=nx.DiGraph())\n",
        "            if type(self.test_indices) != type(None):\n",
        "                self.g.remove_edges_from(list(self.g.in_edges(test_indices)))\n",
        "                self.g.remove_edges_from(list(self.g.out_edges(test_indices)))\n",
        "\n",
        "            self.num_of_nodes = self.g.number_of_nodes()\n",
        "            self.num_of_edges = self.g.number_of_edges()\n",
        "            self.edges_raw = self.g.edges(data=True)\n",
        "            self.nodes_raw = self.g.nodes(data=True)\n",
        "\n",
        "            #self.edge_distribution = np.array([attr['weight'] for _, _, attr in self.edges_raw], dtype=np.float32)\n",
        "            self.edge_distribution = np.array([1/attr['weight'] if attr['weight']>0 else 0 for _, _, attr in self.edges_raw], dtype=np.float32)\n",
        "            self.edge_distribution /= np.sum(self.edge_distribution)\n",
        "            self.edge_sampling = AliasSampling(prob=self.edge_distribution)\n",
        "            ''' self.node_negative_distribution = np.power(\n",
        "                np.array([self.g.degree(node, weight='weight') for node, _ in self.nodes_raw], dtype=np.float32), 0.75) '''\n",
        "\n",
        "            if type(node_negative_distribution_temp) != type(None):\n",
        "                self.node_negative_distribution_temp = node_negative_distribution_temp\n",
        "                #HERE WE HAVE TO BUILD TWO DIFFERENT NODE NEGATIVE SAMPLER\n",
        "                sample_node0, sample_node1, _ = list(self.edges_raw)[0]\n",
        "                node_type1, node_type2 = determine_type(sample_node0), determine_type(sample_node1)\n",
        "                #print(\"node types: \"+str(node_type1)+\" \"+str(node_type2))\n",
        "\n",
        "                node_type1_start, node_type1_end = node_trackers[node_type1]\n",
        "                node_type2_start, node_type2_end = node_trackers[node_type2]\n",
        "\n",
        "                self.node_negative_distribution_temp_type1 = copy.deepcopy(self.node_negative_distribution_temp)\n",
        "                self.node_negative_distribution_temp_type1[0: node_type1_start] = 0\n",
        "                self.node_negative_distribution_temp_type1[node_type1_end: ] = 0\n",
        "                print(\"sum 1 \"+str(np.sum(self.node_negative_distribution_temp_type1)))\n",
        "                \n",
        "                if(np.sum(self.node_negative_distribution_temp_type1) == 0):\n",
        "                  print(\"ZERO DIVIDE WARNING!!! \")\n",
        "                self.node_negative_distribution_type1 = self.node_negative_distribution_temp_type1/np.sum(self.node_negative_distribution_temp_type1)\n",
        "                print(self.node_negative_distribution_temp_type1)\n",
        "                self.node_sampling_type1 = AliasSampling(prob=self.node_negative_distribution_type1)\n",
        "\n",
        "                self.node_negative_distribution_temp_type2 = copy.deepcopy(self.node_negative_distribution_temp)\n",
        "                self.node_negative_distribution_temp_type2[0: node_type2_start] = 0\n",
        "                self.node_negative_distribution_temp_type2[node_type2_end: ] = 0\n",
        "                print(\"sum 2 \"+str(np.sum(self.node_negative_distribution_temp_type2)))\n",
        "\n",
        "                self.node_negative_distribution_type2 = self.node_negative_distribution_temp_type2/np.sum(self.node_negative_distribution_temp_type2)\n",
        "                self.node_sampling_type2 = AliasSampling(prob=self.node_negative_distribution_type2)\n",
        "\n",
        "            else:\n",
        "                print(\"Calculating global graph node properties...\")\n",
        "                g_temp = self.g.to_undirected().to_directed()\n",
        "                print(g_temp.degree(0))\n",
        "                self.node_negative_distribution_temp = np.power(\n",
        "                    np.array([1/g_temp.degree(node, weight='weight') if g_temp.degree(node, weight='weight')>0 else 0 for node, _ in self.nodes_raw], dtype=np.float32), 0.75)\n",
        "                ''' print(self.node_negative_distribution_temp[0:218])\n",
        "                print(\"**********************\")\n",
        "                print(self.node_negative_distribution_temp[218:13557])\n",
        "                print(\"**********************\")\n",
        "                print(self.node_negative_distribution_temp[13557:14266])\n",
        "                print(\"**********************\")\n",
        "                print(self.node_negative_distribution_temp[14266:])\n",
        "                print(\"++++++++++++++++++++++++++++++++++++++++++++++=====================\")\n",
        " '''\n",
        "                self.node_negative_distribution = self.node_negative_distribution_temp/np.sum(self.node_negative_distribution_temp)\n",
        "                self.node_sampling = AliasSampling(prob=self.node_negative_distribution)\n",
        "\n",
        "            ''' if node_negative_distribution_temp.all() == None:\n",
        "                print(\"Calculating global graph node properties...\")\n",
        "                self.node_negative_distribution_temp = np.power(\n",
        "                    np.array([1/self.g.degree(node, weight='weight') if self.g.degree(node, weight='weight')>0 else 0 for node, _ in self.nodes_raw], dtype=np.float32), 0.75)\n",
        "\n",
        "                self.node_negative_distribution = self.node_negative_distribution_temp/np.sum(self.node_negative_distribution_temp)\n",
        "                self.node_sampling = AliasSampling(prob=self.node_negative_distribution)\n",
        "                \n",
        "            else:\n",
        "                self.node_negative_distribution_temp = node_negative_distribution_temp\n",
        "                #HERE WE HAVE TO BUILD TWO DIFFERENT NODE NEGATIVE SAMPLER\n",
        "                sample_node0, sample_node1, _ = self.edges_raw[0]\n",
        "                node_type1, node_type2 = determine_type(sample_node0), determine_type(sample_node1)\n",
        "\n",
        "                node_type1_start, node_type1_end = node_trackers[node_type1]\n",
        "                node_type2_start, node_type2_end = node_trackers[node_type2]\n",
        "\n",
        "                self.node_negative_distribution_temp_type1 = copy.deepcopy(self.node_negative_distribution_temp)\n",
        "                self.node_negative_distribution_temp_type1[0: node_type1_start] = 0\n",
        "                self.node_negative_distribution_temp_type1[node_type1_end: ] = 0\n",
        "                self.node_negative_distribution_type1 = self.node_negative_distribution_temp_type1/np.sum(self.node_negative_distribution_temp_type1)\n",
        "                self.node_sampling_type1 = AliasSampling(prob=self.node_negative_distribution_type1)\n",
        "\n",
        "                self.node_negative_distribution_temp_type2 = copy.deepcopy(self.node_negative_distribution_temp)\n",
        "                self.node_negative_distribution_temp_type2[0: node_type2_start] = 0\n",
        "                self.node_negative_distribution_temp_type2[node_type2_end: ] = 0\n",
        "                self.node_negative_distribution_type2 = self.node_negative_distribution_temp_type2/np.sum(self.node_negative_distribution_temp_type2)\n",
        "                self.node_sampling_type2 = AliasSampling(prob=self.node_negative_distribution_type2) '''\n",
        "\n",
        "            self.node_index = {}\n",
        "            self.node_index_reversed = {}\n",
        "            for index, (node, _) in enumerate(self.nodes_raw):\n",
        "                if index != node:\n",
        "                  raise Exception(\"Discrepancy!!!!\")\n",
        "                self.node_index[node] = index\n",
        "                self.node_index_reversed[index] = node\n",
        "            self.edges = [(self.node_index[u], self.node_index[v]) for u, v, _ in self.edges_raw]\n",
        "\n",
        "\n",
        "    def fetch_next_batch(self, batch_size=16, K=5):\n",
        "        \n",
        "        edge_batch_index = self.edge_sampling.sampling(batch_size)\n",
        "        \n",
        "        u_i = []\n",
        "        u_j = []\n",
        "        label = []\n",
        "        reverse = False\n",
        "        if np.random.rand() > 0.5:\n",
        "            reverse = True\n",
        "        for edge_index in edge_batch_index:\n",
        "            edge = self.edges[edge_index]\n",
        "            #if self.g.__class__ == nx.Graph:\n",
        "            #    if reverse == True:\n",
        "            #        edge = (edge[1], edge[0])\n",
        "            if reverse == True:\n",
        "              edge = (edge[1], edge[0])\n",
        "            u_i.append(edge[0])\n",
        "            u_j.append(edge[1])\n",
        "            label.append(1)\n",
        "            node_type_1 = determine_type(edge[0])\n",
        "            node_type_2 = determine_type(edge[1])\n",
        "\n",
        "            for i in range(K):\n",
        "                while True:\n",
        "                    negative_node = self.node_sampling_type2.sampling() if reverse == False else self.node_sampling_type1.sampling()\n",
        "                    if determine_type(negative_node) == node_type_2 and (not self.g.has_edge(self.node_index_reversed[negative_node], self.node_index_reversed[edge[0]])):\n",
        "                        break\n",
        "                    \"\"\" elif determine_type(negative_node) != node_type_2:\n",
        "                      raise Exception(\"Problem in node sampling causing processing delay\") \"\"\"\n",
        "                      #print(\"neg type: \" + str(determine_type(negative_node)))\n",
        "                      #print(\"pos type: \" + str(node_type_2))\n",
        "\n",
        "                u_i.append(edge[0])\n",
        "                u_j.append(negative_node)\n",
        "                label.append(-1)\n",
        "\n",
        "        '''if len(np.intersect1d(u_i, self.test_indices)) + len(np.intersect1d(u_j, self.test_indices)) > 0:\n",
        "            raise Exception(\"Error ensuring test set not separate from training. Inductivity couldn't be confirmed.\")'''\n",
        "        \n",
        "        return u_i, u_j, label, node_type_1, node_type_2\n",
        "\n",
        "    def embedding_mapping(self, embedding):\n",
        "        return {node: embedding[self.node_index[node]] for node, _ in self.nodes_raw}"
      ],
      "metadata": {
        "id": "LazY9Ku_Cn8v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AliasSampling:\n",
        "    # Reference: LINE source code from https://github.com/snowkylin/line\n",
        "    # Reference: https://en.wikipedia.org/wiki/Alias_method\n",
        "    def __init__(self, prob):\n",
        "        self.n = len(prob)\n",
        "        self.U = np.array(prob) * self.n\n",
        "        self.K = [i for i in range(len(prob))]\n",
        "        overfull, underfull = [], []\n",
        "        for i, U_i in enumerate(self.U):\n",
        "            if U_i > 1:\n",
        "                overfull.append(i)\n",
        "            elif U_i < 1:\n",
        "                underfull.append(i)\n",
        "        while len(overfull) and len(underfull):\n",
        "            i, j = overfull.pop(), underfull.pop()\n",
        "            self.K[j] = i\n",
        "            self.U[i] = self.U[i] - (1 - self.U[j])\n",
        "            if self.U[i] > 1:\n",
        "                overfull.append(i)\n",
        "            elif self.U[i] < 1:\n",
        "                underfull.append(i)\n",
        "\n",
        "    def sampling(self, n=1):\n",
        "        x = np.random.rand(n)\n",
        "        i = np.floor(self.n * x)\n",
        "        y = self.n * x - i\n",
        "        i = i.astype(np.int32)\n",
        "        res = [i[k] if y[k] < self.U[i[k]] else self.K[i[k]] for k in range(n)]\n",
        "        if n == 1:\n",
        "            return res[0]\n",
        "        else:\n",
        "            return res"
      ],
      "metadata": {
        "id": "pxZ7Q_CWCpz7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split(graph_file, p_test=0.10, p_val=0.05):\n",
        "    with np.load(graph_file, allow_pickle=True) as loader:\n",
        "        loader = dict(loader)\n",
        "        A = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
        "                           loader['adj_indptr']), shape=loader['adj_shape'])\n",
        "\n",
        "        X = sp.csr_matrix((loader['attr_data'], loader['attr_indices'],\n",
        "                           loader['attr_indptr']), shape=loader['attr_shape'])\n",
        "\n",
        "        if 'labels' in loader.keys():\n",
        "            labels = loader['labels']\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        train_ones, val_ones, val_zeros, test_ones, test_zeros = _train_val_test_split_adjacency(A=A, p_test=p_test, p_val=p_val, neg_mul=1, every_node=True, connected=False, undirected=(A != A.T).nnz == 0)\n",
        "        if p_val > 0:\n",
        "            val_edges = np.row_stack((val_ones, val_zeros))\n",
        "            val_ground_truth = A[val_edges[:, 0], val_edges[:, 1]].A1\n",
        "            val_ground_truth = np.where(val_ground_truth > 0, 1, val_ground_truth)\n",
        "        if p_test > 0:\n",
        "            test_edges = np.row_stack((test_ones, test_zeros))\n",
        "            test_ground_truth = A[test_edges[:, 0], test_edges[:, 1]].A1\n",
        "            test_ground_truth = np.where(test_ground_truth > 0, 1, test_ground_truth)\n",
        "            if p_val == 0:\n",
        "                val_edges = test_edges\n",
        "                val_ground_truth = test_ground_truth\n",
        "        A = edges_to_sparse(train_ones, A.shape[0])\n",
        "    return A, X, labels, val_edges, val_ground_truth, test_edges, test_ground_truth\n"
      ],
      "metadata": {
        "id": "FLW5BRhsCtHn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _train_val_test_split_adjacency(A, p_val=0.10, p_test=0.05, seed=0, neg_mul=1,\n",
        "                                    every_node=True, connected=False, undirected=False,\n",
        "                                    use_edge_cover=True, set_ops=True, asserts=False):\n",
        "    # Reference: G2G source code from https://github.com/abojchevski/graph2gauss\n",
        "    assert p_val + p_test > 0\n",
        "    assert A.min() == 0  # no negative edges\n",
        "    assert A.diagonal().sum() == 0  # no self-loops\n",
        "    assert not np.any(A.sum(0).A1 + A.sum(1).A1 == 0)  # no dangling nodes\n",
        "    is_undirected = (A != A.T).nnz == 0\n",
        "\n",
        "    if undirected:\n",
        "        assert is_undirected  # make sure is directed\n",
        "        A = sp.tril(A).tocsr()  #    consider only upper triangular\n",
        "        A.eliminate_zeros()\n",
        "    else:\n",
        "        if is_undirected:\n",
        "            warnings.warn('Graph appears to be undirected. Did you forgot to set undirected=True?')\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    E = A.nnz\n",
        "    N = A.shape[0]\n",
        "    s_train = int(E * (1 - p_val - p_test))\n",
        "\n",
        "    idx = np.arange(N)\n",
        "\n",
        "    # hold some edges so each node appears at least once\n",
        "    if every_node:\n",
        "        if connected:\n",
        "            assert sp.csgraph.connected_components(A)[0] == 1  # make sure original graph is connected\n",
        "            A_hold = sp.csgraph.minimum_spanning_tree(A)\n",
        "        else:\n",
        "            A.eliminate_zeros()  # makes sure A.tolil().rows contains only indices of non-zero elements\n",
        "            d = A.sum(1).A1\n",
        "\n",
        "            if use_edge_cover:\n",
        "                hold_edges = edge_cover(A)\n",
        "\n",
        "                # make sure the training percentage is not smaller than len(edge_cover)/E when every_node is set to True\n",
        "                min_size = hold_edges.shape[0]\n",
        "                if min_size > s_train:\n",
        "                    raise ValueError('Training percentage too low to guarantee every node. Min train size needed {:.2f}'\n",
        "                                     .format(min_size / E))\n",
        "            else:\n",
        "                # make sure the training percentage is not smaller than N/E when every_node is set to True\n",
        "                if N > s_train:\n",
        "                    raise ValueError('Training percentage too low to guarantee every node. Min train size needed {:.2f}'\n",
        "                                     .format(N / E))\n",
        "\n",
        "                hold_edges_d1 = np.column_stack(\n",
        "                    (idx[d > 0], np.row_stack(map(np.random.choice, A[d > 0].tolil().rows))))\n",
        "\n",
        "                if np.any(d == 0):\n",
        "                    hold_edges_d0 = np.column_stack((np.row_stack(map(np.random.choice, A[:, d == 0].T.tolil().rows)),\n",
        "                                                     idx[d == 0]))\n",
        "                    hold_edges = np.row_stack((hold_edges_d0, hold_edges_d1))\n",
        "                else:\n",
        "                    hold_edges = hold_edges_d1\n",
        "\n",
        "            if asserts:\n",
        "                assert np.all(A[hold_edges[:, 0], hold_edges[:, 1]])\n",
        "                assert len(np.unique(hold_edges.flatten())) == N\n",
        "\n",
        "            A_hold = edges_to_sparse(hold_edges, N)\n",
        "\n",
        "        A_hold[A_hold > 1] = 1\n",
        "        A_hold.eliminate_zeros()\n",
        "        A_sample = A - A_hold\n",
        "\n",
        "        s_train = s_train - A_hold.nnz\n",
        "    else:\n",
        "        A_sample = A\n",
        "\n",
        "    idx_ones = np.random.permutation(A_sample.nnz)\n",
        "    ones = np.column_stack(A_sample.nonzero())\n",
        "    train_ones = ones[idx_ones[:s_train]]\n",
        "    test_ones = ones[idx_ones[s_train:]]\n",
        "\n",
        "    # return back the held edges\n",
        "    if every_node:\n",
        "        train_ones = np.row_stack((train_ones, np.column_stack(A_hold.nonzero())))\n",
        "\n",
        "    n_test = len(test_ones) * neg_mul\n",
        "    if set_ops:\n",
        "        # generate slightly more completely random non-edge indices than needed and discard any that hit an edge\n",
        "        # much faster compared a while loop\n",
        "        # in the future: estimate the multiplicity (currently fixed 1.3/2.3) based on A_obs.nnz\n",
        "        if undirected:\n",
        "            random_sample = np.random.randint(0, N, [int(2.3 * n_test), 2])\n",
        "            random_sample = random_sample[random_sample[:, 0] > random_sample[:, 1]] #only upper triangle\n",
        "        else:\n",
        "            random_sample = np.random.randint(0, N, [int(1.3 * n_test), 2])\n",
        "            random_sample = random_sample[random_sample[:, 0] != random_sample[:, 1]]\n",
        "\n",
        "        # discard ones\n",
        "        random_sample = random_sample[A[random_sample[:, 0], random_sample[:, 1]].A1 == 0]\n",
        "        # discard duplicates\n",
        "        random_sample = random_sample[np.unique(random_sample[:, 0] * N + random_sample[:, 1], return_index=True)[1]]\n",
        "        # only take as much as needed\n",
        "        test_zeros = np.row_stack(random_sample)[:n_test]\n",
        "        assert test_zeros.shape[0] == n_test\n",
        "    else:\n",
        "        test_zeros = []\n",
        "        while len(test_zeros) < n_test:\n",
        "            i, j = np.random.randint(0, N, 2)\n",
        "            if A[i, j] == 0 and (not undirected or i > j) and (i, j) not in test_zeros:\n",
        "                test_zeros.append((i, j))\n",
        "        test_zeros = np.array(test_zeros)\n",
        "\n",
        "    # split the test set into validation and test set\n",
        "    s_val_ones = int(len(test_ones) * p_val / (p_val + p_test))\n",
        "    s_val_zeros = int(len(test_zeros) * p_val / (p_val + p_test))\n",
        "\n",
        "    val_ones = test_ones[:s_val_ones]\n",
        "    test_ones = test_ones[s_val_ones:]\n",
        "\n",
        "    val_zeros = test_zeros[:s_val_zeros]\n",
        "    test_zeros = test_zeros[s_val_zeros:]\n",
        "\n",
        "    if undirected:\n",
        "        # put (j, i) edges for every (i, j) edge in the respective sets and form back original A\n",
        "        symmetrize = lambda x: np.row_stack((x, np.column_stack((x[:, 1], x[:, 0]))))\n",
        "        train_ones = symmetrize(train_ones)\n",
        "        val_ones = symmetrize(val_ones)\n",
        "        val_zeros = symmetrize(val_zeros)\n",
        "        test_ones = symmetrize(test_ones)\n",
        "        test_zeros = symmetrize(test_zeros)\n",
        "        A = A.maximum(A.T)\n",
        "\n",
        "    if asserts:\n",
        "        set_of_train_ones = set(map(tuple, train_ones))\n",
        "        assert train_ones.shape[0] + test_ones.shape[0] + val_ones.shape[0] == A.nnz\n",
        "        assert (edges_to_sparse(np.row_stack((train_ones, test_ones, val_ones)), N) != A).nnz == 0\n",
        "        assert set_of_train_ones.intersection(set(map(tuple, test_ones))) == set()\n",
        "        assert set_of_train_ones.intersection(set(map(tuple, val_ones))) == set()\n",
        "        assert set_of_train_ones.intersection(set(map(tuple, test_zeros))) == set()\n",
        "        assert set_of_train_ones.intersection(set(map(tuple, val_zeros))) == set()\n",
        "        assert len(set(map(tuple, test_zeros))) == len(test_ones) * neg_mul\n",
        "        assert len(set(map(tuple, val_zeros))) == len(val_ones) * neg_mul\n",
        "        assert not connected or sp.csgraph.connected_components(A_hold)[0] == 1\n",
        "        assert not every_node or ((A_hold - A) > 0).sum() == 0\n",
        "\n",
        "    return train_ones, val_ones, val_zeros, test_ones, test_zeros"
      ],
      "metadata": {
        "id": "aUvYTtRlCxui"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_cover(A):\n",
        "    # Reference: G2G source code from https://github.com/abojchevski/graph2gauss\n",
        "    N = A.shape[0]\n",
        "    d_in = A.sum(0).A1\n",
        "    d_out = A.sum(1).A1\n",
        "\n",
        "    # make sure to include singleton nodes (nodes with one incoming or one outgoing edge)\n",
        "    one_in = np.where((d_in == 1) & (d_out == 0))[0]\n",
        "    one_out = np.where((d_in == 0) & (d_out == 1))[0]\n",
        "\n",
        "    edges = []\n",
        "    edges.append(np.column_stack((A[:, one_in].argmax(0).A1, one_in)))\n",
        "    edges.append(np.column_stack((one_out, A[one_out].argmax(1).A1)))\n",
        "    edges = np.row_stack(edges)\n",
        "\n",
        "    edge_cover_set = set(map(tuple, edges))\n",
        "    nodes = set(edges.flatten())\n",
        "\n",
        "    # greedly add other edges such that both end-point are not yet in the edge_cover_set\n",
        "    cands = np.column_stack(A.nonzero())\n",
        "    for u, v in cands[d_in[cands[:, 1]].argsort()]:\n",
        "        if u not in nodes and v not in nodes and u != v:\n",
        "            edge_cover_set.add((u, v))\n",
        "            nodes.add(u)\n",
        "            nodes.add(v)\n",
        "        if len(nodes) == N:\n",
        "            break\n",
        "\n",
        "    # add a single edge for the rest of the nodes not covered so far\n",
        "    not_covered = np.setdiff1d(np.arange(N), list(nodes))\n",
        "    edges = [list(edge_cover_set)]\n",
        "    not_covered_out = not_covered[d_out[not_covered] > 0]\n",
        "\n",
        "    if len(not_covered_out) > 0:\n",
        "        edges.append(np.column_stack((not_covered_out, A[not_covered_out].argmax(1).A1)))\n",
        "\n",
        "    not_covered_in = not_covered[d_out[not_covered] == 0]\n",
        "    if len(not_covered_in) > 0:\n",
        "        edges.append(np.column_stack((A[:, not_covered_in].argmax(0).A1, not_covered_in)))\n",
        "\n",
        "    edges = np.row_stack(edges)\n",
        "\n",
        "    # make sure that we've indeed computed an edge_cover\n",
        "    # assert A[edges[:, 0], edges[:, 1]].sum() == len(edges)\n",
        "    assert len(set(map(tuple, edges))) == len(edges)\n",
        "    assert len(np.unique(edges)) == N\n",
        "\n",
        "    return edges"
      ],
      "metadata": {
        "id": "t1r3WlziC2iN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edges_to_sparse(edges, N, values=None):\n",
        "    if values is None:\n",
        "        values = np.ones(edges.shape[0])\n",
        "\n",
        "    return sp.coo_matrix((values, (edges[:, 0], edges[:, 1])), shape=(N, N)).tocsr()"
      ],
      "metadata": {
        "id": "W2-6LVQZC5DD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_link_prediction(labels, scores):\n",
        "    return roc_auc_score(labels, scores), average_precision_score(labels, scores)"
      ],
      "metadata": {
        "id": "e4FcR0XzC51h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.py"
      ],
      "metadata": {
        "id": "pW0YNrlzDMAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q -U tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkFPQOEfDlIW",
        "outputId": "d8cbf4c2-f47d-4b43-db64-d94a2fa2ab4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 768 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 798 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 880 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 890 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 911 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 921 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 942 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 952 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 972 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 983 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import scipy.sparse as sp \n",
        "import numpy as np \n",
        "#import cyclic_learning_rate.clr as clr\n",
        "\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "UmGaZko4DRqo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_feeder(M):\n",
        "    M = sp.coo_matrix(M, dtype = np.float32)\n",
        "    return np.vstack((M.row, M.col)).T, M.data, M.shape"
      ],
      "metadata": {
        "id": "fV9ab4qnFmBo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GSNE:\n",
        "    def __init__(self, args):\n",
        "        tf.set_random_seed(seed)\n",
        "        self.X1 = tf.SparseTensor(*sparse_feeder(args.X1))\n",
        "        self.X2 = tf.SparseTensor(*sparse_feeder(args.X2))\n",
        "        self.X3 = tf.SparseTensor(*sparse_feeder(args.X3))\n",
        "        self.X4 = tf.SparseTensor(*sparse_feeder(args.X4))\n",
        "\n",
        "        self.N1, self.D1 = args.X1.shape\n",
        "        self.N2, self.D2 = args.X2.shape\n",
        "        self.N3, self.D3 = args.X3.shape\n",
        "        self.N4, self.D4 = args.X4.shape  \n",
        "\n",
        "        self.L = args.embedding_dim\n",
        "\n",
        "        #PLEASE ENSURE THE LAST LAYER DIMENSION IS SAME FOR EVERYONE\n",
        "\n",
        "        self.n_hidden1 = [6, 12, 28]\n",
        "        self.n_hidden2 = [10, 16, 28]\n",
        "        self.n_hidden3 = [10, 16, 28]\n",
        "        self.n_hidden4 = [42, 36, 28]\n",
        "\n",
        "        '''self.n_hidden1 = [14]\n",
        "        self.n_hidden2 = [14]\n",
        "        self.n_hidden3 = [14]\n",
        "        self.n_hidden4 = [14]'''\n",
        "\n",
        "        self.u_i = tf.placeholder(name='u_i', dtype=tf.int32, shape=[args.batch_size * (args.K + 1)])\n",
        "        self.u_j = tf.placeholder(name='u_j', dtype=tf.int32, shape=[args.batch_size * (args.K + 1)])\n",
        "        self.label = tf.placeholder(name='label', dtype=tf.float32, shape=[args.batch_size * (args.K + 1)])\n",
        "        self.node_type1 = tf.placeholder(name='node_type1', dtype=tf.int32, shape = ())\n",
        "        self.node_type2 = tf.placeholder(name='node_type2', dtype=tf.int32, shape = ())\n",
        "\n",
        "        self.__create_model(args.proximity)\n",
        "        self.val_set = False\n",
        "        \n",
        "        tf.train.create_global_step()\n",
        "\n",
        "        # softmax loss\n",
        "        \n",
        "        self.energy = -self.energy_kl(self.u_i, self.u_j, args.proximity, self.node_type1, self.node_type2)\n",
        "        self.loss = -tf.reduce_mean(tf.log_sigmoid(self.label * self.energy))\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        print(args.learning_rate)\n",
        "\n",
        "        '''for cyclic learning rate'''\n",
        "        global_step = tf.train.get_global_step()\n",
        "        #learning_rate = tf.train.exponential_decay((1e-9), global_step=global_step,decay_steps=10, decay_rate=1.04)\n",
        "        learning_rate = clr.cyclic_learning_rate(global_step=global_step, learning_rate=1e-4,\n",
        "                         max_lr=19e-5,\n",
        "                         step_size=100, mode='exp_range', gamma = 0.99999)\n",
        "        original_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "        tf.summary.scalar('learning_rate', learning_rate)\n",
        "        tf.summary.scalar(\"current_step\",global_step)\n",
        "\n",
        "\n",
        "        ###########################################################\n",
        "\n",
        "\n",
        "        #original_optimizer = tf.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "        self.optimizer = tf.contrib.estimator.clip_gradients_by_norm(original_optimizer, clip_norm=5.0)\n",
        "\n",
        "        \n",
        "\n",
        "        ''' tvs = tf.trainable_variables()\n",
        "        accum_vars = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]                                        \n",
        "        self.zero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
        "        gvs = self.optimizer.compute_gradients(self.loss, tvs)\n",
        "        self.accum_ops = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]\n",
        "\n",
        "        #After getting all the gradients in the five steps, we calculate the train step\n",
        "        self.train_step = self.optimizer.apply_gradients([(accum_vars[i], gv[1]) for i, gv in enumerate(gvs)])\n",
        " '''\n",
        "        self.train_op = self.optimizer.minimize(self.loss, global_step = global_step)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "        \n",
        "\n",
        "    def __create_model(self, proximity):\n",
        "        w_init = tf.contrib.layers.xavier_initializer\n",
        "        #w_init = tf.random_normal_initializer(mean=0.0, stddev=0.1, seed=None)\n",
        "        #w_init = tf.keras.initializers.random_normal\n",
        "        sizes1 = [self.D1] + self.n_hidden1\n",
        "        sizes2 = [self.D2] + self.n_hidden2\n",
        "        sizes3 = [self.D3] + self.n_hidden3\n",
        "        sizes4 = [self.D4] + self.n_hidden4\n",
        "\n",
        "        #feature 1\n",
        "        TRAINABLE = True\n",
        "        with tf.name_scope(\"Train\"):\n",
        "          for i in range(1, len(sizes1)):\n",
        "             with tf.name_scope(\"enc{}\".format(i)):\n",
        "              W = tf.get_variable(name='W1{}'.format(i), shape=[sizes1[i - 1], sizes1[i]], dtype=tf.float32,\n",
        "                                  initializer=w_init(), trainable = TRAINABLE)\n",
        "              b = tf.get_variable(name='b1{}'.format(i), shape=[sizes1[i]], dtype=tf.float32, initializer=w_init(), trainable = TRAINABLE)\n",
        "\n",
        "              if i == 1:\n",
        "                  encoded1 = tf.sparse_tensor_dense_matmul(self.X1, W) + b\n",
        "              else:\n",
        "                  encoded1 = tf.matmul(encoded1, W) + b\n",
        "\n",
        "              encoded1 = tf.nn.relu(encoded1)\n",
        "\n",
        "              tf.summary.histogram('Weight', W)\n",
        "              tf.summary.histogram('bias', b)\n",
        "              tf.summary.histogram('activations', encoded1)\n",
        "\n",
        "\n",
        "        #encoded1 = tf.Print(encoded1, [encoded1], message = \"feature 1 encoder triggered\")\n",
        "\n",
        "        #feature 2\n",
        "        TRAINABLE = True\n",
        "        with tf.name_scope(\"Region\"):\n",
        "          for i in range(1, len(sizes2)):\n",
        "            with tf.name_scope(\"enc{}\".format(i)):\n",
        "              W = tf.get_variable(name='W2{}'.format(i), shape=[sizes2[i - 1], sizes2[i]], dtype=tf.float32,\n",
        "                                  initializer=w_init(), trainable = TRAINABLE)\n",
        "              b = tf.get_variable(name='b2{}'.format(i), shape=[sizes2[i]], dtype=tf.float32, initializer=w_init(), trainable = TRAINABLE)\n",
        "\n",
        "              if i == 1:\n",
        "                  encoded2 = tf.sparse_tensor_dense_matmul(self.X2, W) + b\n",
        "              else:\n",
        "                  encoded2 = tf.matmul(encoded2, W) + b\n",
        "\n",
        "              encoded2 = tf.nn.relu(encoded2)\n",
        "\n",
        "              tf.summary.histogram('Weight', W)\n",
        "              tf.summary.histogram('bias', b)\n",
        "              tf.summary.histogram('activations', encoded2)\n",
        "\n",
        "        #encoded2 = tf.Print(encoded2, [encoded2], message = \"feature 2 encoder triggered\")\n",
        "\n",
        "        #feature 3\n",
        "        TRAINABLE = True\n",
        "        with tf.name_scope(\"School\"):\n",
        "          for i in range(1, len(sizes3)):\n",
        "            with tf.name_scope(\"enc{}\".format(i)):\n",
        "              W = tf.get_variable(name='W3{}'.format(i), shape=[sizes3[i - 1], sizes3[i]], dtype=tf.float32,\n",
        "                                  initializer=w_init(), trainable = TRAINABLE)\n",
        "              b = tf.get_variable(name='b3{}'.format(i), shape=[sizes3[i]], dtype=tf.float32, initializer=w_init(), trainable = TRAINABLE)\n",
        "\n",
        "              if i == 1:\n",
        "                  encoded3 = tf.sparse_tensor_dense_matmul(self.X3, W) + b\n",
        "              else:\n",
        "                  encoded3 = tf.matmul(encoded3, W) + b\n",
        "\n",
        "              encoded3 = tf.nn.relu(encoded3)\n",
        "\n",
        "              tf.summary.histogram('Weight', W)\n",
        "              tf.summary.histogram('bias', b)\n",
        "              tf.summary.histogram('activations', encoded3)\n",
        "\n",
        "\n",
        "        #encoded3 = tf.Print(encoded3, [encoded3], message = \"feature 3 encoder triggered\")\n",
        "\n",
        "        #feature 4\n",
        "        TRAINABLE = True\n",
        "        with tf.name_scope(\"House\"):\n",
        "          for i in range(1, len(sizes4)):\n",
        "            with tf.name_scope(\"enc{}\".format(i)):\n",
        "              W = tf.get_variable(name='W4{}'.format(i), shape=[sizes4[i - 1], sizes4[i]], dtype=tf.float32,\n",
        "                                  initializer=w_init(), trainable = TRAINABLE)\n",
        "              b = tf.get_variable(name='b4{}'.format(i), shape=[sizes4[i]], dtype=tf.float32, initializer=w_init(), trainable = TRAINABLE)\n",
        "\n",
        "              if i == 1:\n",
        "                  encoded4 = tf.sparse_tensor_dense_matmul(self.X4, W) + b\n",
        "              else:\n",
        "                  encoded4 = tf.matmul(encoded4, W) + b\n",
        "\n",
        "              encoded4 = tf.nn.relu(encoded4)\n",
        "\n",
        "              tf.summary.histogram('Weight', W)\n",
        "              tf.summary.histogram('bias', b)\n",
        "              tf.summary.histogram('activations', encoded4)\n",
        "\n",
        "        #encoded4 = tf.Print(encoded4, [encoded4], message = \"feature 4 encoder triggered\")\n",
        "\n",
        "        #W-MU/SIGMA AND B-MU/SIGMA IS SHARED BETWEEN ALL FEATURES\n",
        "        #SHAPE: THOUGH WE USED SIZES1[-1], KEEP IN MIND THAT'S SAME FOR ALL SHAPES\n",
        "\n",
        "        \"\"\" W_mu = tf.get_variable(name='W_mu', shape=[sizes1[-1], self.L], dtype=tf.float32, initializer=w_init())\n",
        "        b_mu = tf.get_variable(name='b_mu', shape=[self.L], dtype=tf.float32, initializer=w_init()) \n",
        "        self.embedding1 = tf.matmul(encoded1, W_mu) + b_mu\n",
        "        self.embedding2 = tf.matmul(encoded2, W_mu) + b_mu\n",
        "        self.embedding3 = tf.matmul(encoded3, W_mu) + b_mu\n",
        "        self.embedding4 = tf.matmul(encoded4, W_mu) + b_mu\n",
        "\n",
        "        with tf.name_scope(\"shared\"):\n",
        "          with tf.name_scope(\"mu\"):\n",
        "            tf.summary.histogram('Weight', W_mu)\n",
        "            tf.summary.histogram('bias', b_mu)\n",
        "            mu_embed_activations = [self.embedding1, self.embedding2, self.embedding3, self.embedding4]\n",
        "            tf.summary.histogram('activations', tf.concat(mu_embed_activations, 0)) \"\"\"\n",
        "\n",
        "      \n",
        "        '''self.embedding1 = tf.nn.sigmoid(tf.matmul(encoded1, W_mu) + b_mu) + 1 + 1e-14\n",
        "        self.embedding2 = tf.nn.sigmoid(tf.matmul(encoded2, W_mu) + b_mu) + 1 + 1e-14\n",
        "        self.embedding3 = tf.nn.sigmoid(tf.matmul(encoded3, W_mu) + b_mu) + 1 + 1e-14\n",
        "        self.embedding4 = tf.nn.sigmoid(tf.matmul(encoded4, W_mu) + b_mu) + 1 + 1e-14'''\n",
        "\n",
        "        \"\"\" W_sigma = tf.get_variable(name='W_sigma', shape=[sizes1[-1], self.L], dtype=tf.float32, initializer=w_init())\n",
        "        b_sigma = tf.get_variable(name='b_sigma', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "        log_sigma1 = tf.matmul(encoded1, W_sigma) + b_sigma\n",
        "        self.sigma1 = tf.nn.elu(log_sigma1) + 1 + 1e-14\n",
        "        #self.sigma1 = tf.nn.sigmoid(log_sigma1) + 1 + 1e-14\n",
        "\n",
        "        log_sigma2 = tf.matmul(encoded2, W_sigma) + b_sigma\n",
        "        self.sigma2 = tf.nn.elu(log_sigma2) + 1 + 1e-14\n",
        "        #self.sigma2 = tf.nn.sigmoid(log_sigma2) + 1 + 1e-14\n",
        "\n",
        "        log_sigma3 = tf.matmul(encoded3, W_sigma) + b_sigma\n",
        "        self.sigma3 = tf.nn.elu(log_sigma3) + 1 + 1e-14\n",
        "        #self.sigma3 = tf.nn.sigmoid(log_sigma3) + 1 + 1e-14\n",
        "\n",
        "        log_sigma4 = tf.matmul(encoded4, W_sigma) + b_sigma\n",
        "        self.sigma4 = tf.nn.elu(log_sigma4) + 1 + 1e-14\n",
        "        #self.sigma4 = tf.nn.sigmoid(log_sigma4) + 1 + 1e-14\n",
        "\n",
        "        with tf.name_scope(\"shared\"):\n",
        "          with tf.name_scope(\"sigma\"):\n",
        "            tf.summary.histogram('Weight', W_sigma)\n",
        "            tf.summary.histogram('bias', b_sigma)\n",
        "            sigma_embed_activations = [self.sigma1, self.sigma2, self.sigma3, self.sigma4]\n",
        "            tf.summary.histogram('activations', tf.concat(sigma_embed_activations, 0)) \"\"\" \n",
        "\n",
        "        ##############EXPERIMENTAL FEATURES. PLEASE REMOVE IF DOESN'T WORK############################################\n",
        "\n",
        "        W_mu1 = tf.get_variable(name='W_mu1', shape=[sizes1[-1], 40], dtype=tf.float32, initializer=w_init())\n",
        "        b_mu1 = tf.get_variable(name='b_mu1', shape=[40], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "        W_mu2 = tf.get_variable(name='W_mu2', shape=[40, self.L], dtype=tf.float32, initializer=w_init())\n",
        "        b_mu2 = tf.get_variable(name='b_mu2', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "        \n",
        "        embedding1_t = tf.nn.relu(tf.matmul(encoded1, W_mu1) + b_mu1)\n",
        "        self.embedding1 = tf.matmul(embedding1_t, W_mu2) + b_mu2\n",
        "\n",
        "        embedding2_t = tf.nn.relu(tf.matmul(encoded2, W_mu1) + b_mu1)\n",
        "        self.embedding2 = tf.matmul(embedding2_t, W_mu2) + b_mu2\n",
        "\n",
        "        embedding3_t = tf.nn.relu(tf.matmul(encoded3, W_mu1) + b_mu1)\n",
        "        self.embedding3 = tf.matmul(embedding3_t, W_mu2) + b_mu2\n",
        "\n",
        "        embedding4_t = tf.nn.relu(tf.matmul(encoded4, W_mu1) + b_mu1)\n",
        "        self.embedding4 = tf.matmul(embedding4_t, W_mu2) + b_mu2\n",
        "\n",
        "        W_sigma1 = tf.get_variable(name='W_sigma1', shape=[sizes1[-1], 40], dtype=tf.float32, initializer=w_init())\n",
        "        W_sigma2 = tf.get_variable(name='W_sigma2', shape=[40, self.L], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "        b_sigma1 = tf.get_variable(name='b_sigma1', shape=[40], dtype=tf.float32, initializer=w_init())\n",
        "        b_sigma2 = tf.get_variable(name='b_sigma2', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "        log_sigma1t = tf.nn.relu(tf.matmul(encoded1, W_sigma1) + b_sigma1)\n",
        "        log_sigma1 = tf.matmul(log_sigma1t, W_sigma2) + b_sigma2\n",
        "        self.sigma1 = tf.nn.elu(log_sigma1) + 1 + 1e-14\n",
        "        #self.sigma1 = tf.nn.sigmoid(log_sigma1) + 1 + 1e-14\n",
        "\n",
        "        log_sigma2t = tf.nn.relu(tf.matmul(encoded2, W_sigma1) + b_sigma1)\n",
        "        log_sigma2 = tf.matmul(log_sigma2t, W_sigma2) + b_sigma2\n",
        "        self.sigma2 = tf.nn.elu(log_sigma2) + 1 + 1e-14\n",
        "        #self.sigma2 = tf.nn.sigmoid(log_sigma2) + 1 + 1e-14\n",
        "\n",
        "        log_sigma3t = tf.nn.relu(tf.matmul(encoded3, W_sigma1) + b_sigma1)\n",
        "        log_sigma3 = tf.matmul(log_sigma3t, W_sigma2) + b_sigma2\n",
        "        self.sigma3 = tf.nn.elu(log_sigma3) + 1 + 1e-14\n",
        "        #self.sigma3 = tf.nn.sigmoid(log_sigma3) + 1 + 1e-14\n",
        "\n",
        "        log_sigma4t = tf.nn.relu(tf.matmul(encoded4, W_sigma1) + b_sigma1)\n",
        "        log_sigma4 = tf.matmul(log_sigma4t, W_sigma2) + b_sigma2\n",
        "        self.sigma4 = tf.nn.elu(log_sigma4) + 1 + 1e-14 \n",
        "        #self.sigma4 = tf.nn.sigmoid(log_sigma4) + 1 + 1e-14\n",
        "\n",
        "        ########################################################################################################################\n",
        "        #####################END OF EXPERIMENTAL, DELETE IF DOESN'T WORK########################################################\n",
        "        #######################################################################################################################\n",
        "        \n",
        "\n",
        "\n",
        "        if proximity == 'second-order':\n",
        "            #feature 1\n",
        "\n",
        "            for i in range(1, len(sizes1)):\n",
        "                W = tf.get_variable(name='W_ctx1{}'.format(i), shape=[sizes1[i - 1], sizes1[i]], dtype=tf.float32,\n",
        "                                    initializer=w_init())\n",
        "                b = tf.get_variable(name='b_ctx1{}'.format(i), shape=[sizes1[i]], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "                if i == 1:\n",
        "                    encoded1 = tf.sparse_tensor_dense_matmul(self.X1, W) + b\n",
        "                else:\n",
        "                    encoded1 = tf.matmul(encoded1, W) + b\n",
        "\n",
        "                encoded1 = tf.nn.relu(encoded1)\n",
        "\n",
        "            #feature 2\n",
        "\n",
        "            for i in range(1, len(sizes2)):\n",
        "                W = tf.get_variable(name='W_ctx2{}'.format(i), shape=[sizes2[i - 1], sizes2[i]], dtype=tf.float32,\n",
        "                                    initializer=w_init())\n",
        "                b = tf.get_variable(name='b_ctx2{}'.format(i), shape=[sizes2[i]], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "                if i == 1:\n",
        "                    encoded2 = tf.sparse_tensor_dense_matmul(self.X2, W) + b\n",
        "                else:\n",
        "                    encoded2 = tf.matmul(encoded2, W) + b\n",
        "\n",
        "                encoded2 = tf.nn.relu(encoded2)\n",
        "\n",
        "            #feature 3\n",
        "\n",
        "            for i in range(1, len(sizes3)):\n",
        "                W = tf.get_variable(name='W_ctx3{}'.format(i), shape=[sizes3[i - 1], sizes3[i]], dtype=tf.float32,\n",
        "                                    initializer=w_init())\n",
        "                b = tf.get_variable(name='b_ctx3{}'.format(i), shape=[sizes3[i]], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "                if i == 1:\n",
        "                    encoded3 = tf.sparse_tensor_dense_matmul(self.X3, W) + b\n",
        "                else:\n",
        "                    encoded3 = tf.matmul(encoded3, W) + b\n",
        "\n",
        "                encoded3 = tf.nn.relu(encoded3)\n",
        "\n",
        "            #feature 4\n",
        "\n",
        "            for i in range(1, len(sizes4)):\n",
        "                W = tf.get_variable(name='W_ctx4{}'.format(i), shape=[sizes4[i - 1], sizes4[i]], dtype=tf.float32,\n",
        "                                    initializer=w_init())\n",
        "                b = tf.get_variable(name='b_ctx4{}'.format(i), shape=[sizes4[i]], dtype=tf.float32, initializer=w_init())\n",
        "\n",
        "                if i == 1:\n",
        "                    encoded4 = tf.sparse_tensor_dense_matmul(self.X4, W) + b\n",
        "                else:\n",
        "                    encoded4 = tf.matmul(encoded4, W) + b\n",
        "\n",
        "                encoded4 = tf.nn.relu(encoded4)\n",
        "            ################ USE INTERCHANGABLY WITH THE HIGHER DIMENSION#####################################################\n",
        "            \"\"\" W_mu = tf.get_variable(name='W_mu_ctx', shape=[sizes1[-1], self.L], dtype=tf.float32, initializer=w_init())\n",
        "            b_mu = tf.get_variable(name='b_mu_ctx', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "            \n",
        "            self.ctx_mu1 = tf.matmul(encoded1, W_mu) + b_mu\n",
        "            self.ctx_mu2 = tf.matmul(encoded2, W_mu) + b_mu\n",
        "            self.ctx_mu3 = tf.matmul(encoded3, W_mu) + b_mu\n",
        "            self.ctx_mu4 = tf.matmul(encoded4, W_mu) + b_mu\n",
        "\n",
        "            ''' self.ctx_mu1 = tf.nn.sigmoid(tf.matmul(encoded1, W_mu) + b_mu) + 1 + 1e-14\n",
        "            self.ctx_mu2 = tf.nn.sigmoid(tf.matmul(encoded2, W_mu) + b_mu) + 1 + 1e-14\n",
        "            self.ctx_mu3 = tf.nn.sigmoid(tf.matmul(encoded3, W_mu) + b_mu) + 1 + 1e-14 \n",
        "\n",
        "            self.ctx_mu4 = tf.nn.sigmoid(tf.matmul(encoded4, W_mu) + b_mu) + 1 + 1e-14 '''\n",
        "\n",
        "            W_sigma = tf.get_variable(name='W_sigma_ctx', shape=[sizes1[-1], self.L], dtype=tf.float32,\n",
        "                                      initializer=w_init())\n",
        "            b_sigma = tf.get_variable(name='b_sigma_ctx', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "            \n",
        "            log_sigma1 = tf.matmul(encoded1, W_sigma) + b_sigma\n",
        "            self.ctx_sigma1 = tf.nn.elu(log_sigma1) + 1 + 1e-14\n",
        "            #self.ctx_sigma1 = tf.nn.sigmoid(log_sigma1) + 1 + 1e-14\n",
        "\n",
        "            log_sigma2 = tf.matmul(encoded2, W_sigma) + b_sigma\n",
        "            self.ctx_sigma2 = tf.nn.elu(log_sigma2) + 1 + 1e-14\n",
        "            #self.ctx_sigma2 = tf.nn.sigmoid(log_sigma2) + 1 + 1e-14\n",
        "\n",
        "            log_sigma3 = tf.matmul(encoded3, W_sigma) + b_sigma\n",
        "            self.ctx_sigma3 = tf.nn.elu(log_sigma3) + 1 + 1e-14\n",
        "            #self.ctx_sigma3 = tf.nn.sigmoid(log_sigma3) + 1 + 1e-14\n",
        "\n",
        "\n",
        "            log_sigma4 = tf.matmul(encoded4, W_sigma) + b_sigma\n",
        "            self.ctx_sigma4 = tf.nn.elu(log_sigma4) + 1 + 1e-14\n",
        "            #self.ctx_sigma4 = tf.nn.sigmoid(log_sigma4) + 1 + 1e-14 \"\"\"\n",
        "\n",
        "            #############HIGHER DIMENSION VERSION ##############################################\n",
        "            W_mu1 = tf.get_variable(name='W_mu_ctx1', shape=[sizes1[-1], 40], dtype=tf.float32, initializer=w_init())\n",
        "            b_mu1 = tf.get_variable(name='b_mu_ctx1', shape=[40], dtype=tf.float32, initializer=w_init())\n",
        "            \n",
        "            W_mu2 = tf.get_variable(name='W_mu_ctx2', shape=[40, self.L], dtype=tf.float32, initializer=w_init())\n",
        "            b_mu2 = tf.get_variable(name='b_mu_ctx2', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "            \n",
        "            ctx_mu1_t = tf.nn.relu(tf.matmul(encoded1, W_mu1) + b_mu1)\n",
        "            self.ctx_mu1 = tf.matmul(ctx_mu1_t, W_mu2) + b_mu2\n",
        "            \n",
        "            ctx_mu2_t = tf.nn.relu(tf.matmul(encoded2, W_mu1) + b_mu1)\n",
        "            self.ctx_mu2 = tf.matmul(ctx_mu2_t, W_mu2) + b_mu2\n",
        "            \n",
        "            ctx_mu3_t = tf.nn.relu(tf.matmul(encoded3, W_mu1) + b_mu1)\n",
        "            self.ctx_mu3 = tf.matmul(ctx_mu3_t, W_mu2) + b_mu2\n",
        "            \n",
        "            ctx_mu4_t = tf.nn.relu(tf.matmul(encoded4, W_mu1) + b_mu1)\n",
        "            self.ctx_mu4 = tf.matmul(ctx_mu4_t, W_mu2) + b_mu2\n",
        "\n",
        "            W_sigma1 = tf.get_variable(name='W_sigma_ctx1', shape=[sizes1[-1], 40], dtype=tf.float32, initializer=w_init())\n",
        "            W_sigma2 = tf.get_variable(name='W_sigma_ctx2', shape=[40, self.L], dtype=tf.float32, initializer=w_init())\n",
        "            \n",
        "            b_sigma1 = tf.get_variable(name='b_sigma_ctx1', shape=[40], dtype=tf.float32, initializer=w_init())\n",
        "            b_sigma2 = tf.get_variable(name='b_sigma_ctx2', shape=[self.L], dtype=tf.float32, initializer=w_init())\n",
        "            \n",
        "            log_sigma1t = tf.nn.relu(tf.matmul(encoded1, W_sigma1) + b_sigma1)\n",
        "            log_sigma1 = tf.matmul(log_sigma1t, W_sigma2) + b_sigma2\n",
        "            self.ctx_sigma1 = tf.nn.elu(log_sigma1) + 1 + 1e-14\n",
        "            #self.ctx_sigma1 = tf.nn.sigmoid(log_sigma1) + 1 + 1e-14\n",
        "\t\t\t\n",
        "            log_sigma2t = tf.nn.relu(tf.matmul(encoded2, W_sigma1) + b_sigma1)\n",
        "            log_sigma2 = tf.matmul(log_sigma2t, W_sigma2) + b_sigma2\n",
        "            self.ctx_sigma2 = tf.nn.elu(log_sigma2) + 1 + 1e-14\n",
        "            #self.ctx_sigma2 = tf.nn.sigmoid(log_sigma2) + 1 + 1e-14\n",
        "\n",
        "            log_sigma3t = tf.nn.relu(tf.matmul(encoded3, W_sigma1) + b_sigma1)\n",
        "            log_sigma3 = tf.matmul(log_sigma3t, W_sigma2) + b_sigma2\n",
        "            self.ctx_sigma3 = tf.nn.elu(log_sigma3) + 1 + 1e-14\n",
        "            #self.ctx_sigma3 = tf.nn.sigmoid(log_sigma3) + 1 + 1e-14\n",
        "\n",
        "            log_sigma4t = tf.nn.relu(tf.matmul(encoded4, W_sigma1) + b_sigma1)\n",
        "            log_sigma4 = tf.matmul(log_sigma4t, W_sigma2) + b_sigma2\n",
        "            self.ctx_sigma4 = tf.nn.elu(log_sigma4) + 1 + 1e-14\n",
        "            #self.ctx_sigma4 = tf.nn.sigmoid(log_sigma4) + 1 + 1e-14\n",
        "            #########################################DEEPER MODEL END##################################\n",
        "\n",
        "\n",
        "    def energy_kl(self, u_i, u_j, proximity, node_type1, node_type2):\n",
        "        def f1():\n",
        "          print(\"f1\") \n",
        "          return tf.gather(self.embedding1, u_i), tf.gather(self.sigma1, u_i)\n",
        "        def f2(): \n",
        "          print(\"f2\")\n",
        "          return tf.gather(self.embedding2, u_i), tf.gather(self.sigma2, u_i)\n",
        "        def f3(): \n",
        "          print(\"f3\")\n",
        "          return tf.gather(self.embedding3, u_i), tf.gather(self.sigma3, u_i)\n",
        "        def f4(): \n",
        "          print(\"f4\")\n",
        "          return tf.gather(self.embedding4, u_i), tf.gather(self.sigma4, u_i)\n",
        "\n",
        "        def f5(): \n",
        "          print(\"f5\")\n",
        "          return tf.gather(self.ctx_mu1, u_j), tf.gather(self.ctx_sigma1, u_j)\n",
        "        def f6(): \n",
        "          print(\"f6\")\n",
        "          return tf.gather(self.ctx_mu2, u_j), tf.gather(self.ctx_sigma2, u_j)\n",
        "        def f7(): \n",
        "          print(\"f7\")\n",
        "          return tf.gather(self.ctx_mu3, u_j), tf.gather(self.ctx_sigma3, u_j)\n",
        "        def f8(): \n",
        "          print(\"f8\")\n",
        "          return tf.gather(self.ctx_mu4, u_j), tf.gather(self.ctx_sigma4, u_j)\n",
        "\n",
        "        def f9():\n",
        "          print(\"f9\") \n",
        "          return tf.gather(self.embedding1, u_j), tf.gather(self.sigma1, u_j)\n",
        "        def f10(): \n",
        "          print(\"f10\")\n",
        "          return tf.gather(self.embedding2, u_j), tf.gather(self.sigma2, u_j)\n",
        "        def f11(): \n",
        "          print(\"f11\")\n",
        "          return tf.gather(self.embedding3, u_j), tf.gather(self.sigma3, u_j)\n",
        "        def f12(): \n",
        "          print(\"f12\")\n",
        "          return tf.gather(self.embedding4, u_j), tf.gather(self.sigma4, u_j)\n",
        "\n",
        "        mu_i, sigma_i = tf.case([(tf.equal(node_type1, 0), f1), (tf.equal(node_type1, 1), f2),\n",
        "                              (tf.equal(node_type1, 2), f3), (tf.equal(node_type1, 3), f4)],\n",
        "         default=None, exclusive=True)\n",
        "        \n",
        "        mu_j, sigma_j = tf.case([(tf.equal(node_type2, 0), f9), (tf.equal(node_type2, 1), f10),\n",
        "                              (tf.equal(node_type2, 2), f11), (tf.equal(node_type2, 3), f12)],\n",
        "         default=None, exclusive=True)\n",
        "\n",
        "        sigma_ratio = sigma_j / sigma_i\n",
        "        trace_fac = tf.reduce_sum(sigma_ratio, 1)\n",
        "        log_det = tf.reduce_sum(tf.log(sigma_ratio + 1e-11), 1)\n",
        "\n",
        "        mu_diff_sq = tf.reduce_sum(tf.square(mu_i - mu_j) / sigma_i, 1)\n",
        "\n",
        "        ij_kl = 0.5 * (trace_fac + mu_diff_sq - self.L - log_det)\n",
        "\n",
        "        sigma_ratio = sigma_i / sigma_j\n",
        "        trace_fac = tf.reduce_sum(sigma_ratio, 1)\n",
        "        log_det = tf.reduce_sum(tf.log(sigma_ratio + 1e-11), 1)\n",
        "\n",
        "        mu_diff_sq = tf.reduce_sum(tf.square(mu_j - mu_i) / sigma_j, 1)\n",
        "\n",
        "        ji_kl = 0.5 * (trace_fac + mu_diff_sq - self.L - log_det)\n",
        "\n",
        "        kl_distance = 0.5 * (ij_kl + ji_kl)\n",
        "\n",
        "        return kl_distance"
      ],
      "metadata": {
        "id": "K-JrzdVcFql4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py"
      ],
      "metadata": {
        "id": "5b_Z6QUQF1I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import argparse\n",
        "#from model_20 import GSNE\n",
        "#from utils_20 import DataUtils, score_link_prediction\n",
        "import pickle\n",
        "import time\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import random\n",
        "import copy\n",
        "from IPython.display import clear_output\n",
        "import random"
      ],
      "metadata": {
        "id": "mB9i0C9PF9DS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node1_start, node1_end = (0, 218)\n",
        "node2_start, node2_end = (218, 13557)\n",
        "node3_start, node3_end = (13557, 14266)\n",
        "node4_start, node4_end = (14266, 67117)"
      ],
      "metadata": {
        "id": "n27qJWBZGKmk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 6521\n",
        "MODEL_ID = 20\n",
        "tensorboard_path = 'House Price Content List/Tensorboard/' + str(MODEL_ID)\n",
        "model_path = 'House Price Content List/Model Params/' + str(MODEL_ID)+'/'\n",
        "embedding_path = 'House Price Content List/Embeddings/' + str(MODEL_ID)+'/'"
      ],
      "metadata": {
        "id": "YPwGkuPyGOgQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_sampled(arrName, pivotColumn, num_of_intervals):\n",
        "    arrName = arrName[~np.isnan(arrName).any(axis=1)]\n",
        "    col = np.array(arrName[pivotColumn])\n",
        "    #print(col)\n",
        "    bins = np.linspace(min(col), max(col), num_of_intervals)\n",
        "    left = 0\n",
        "    right = 1\n",
        "    train = np.array([arrName.values[0]])\n",
        "    test_set = np.array([arrName.values[0]])\n",
        "    train = np.delete(train,(0), axis = 0)\n",
        "    #np.delete(val,(0), axis = 0)\n",
        "    test_set = np.delete(test_set,(0), axis = 0)\n",
        "    \n",
        "    portion_arr = []\n",
        "    while(right < len(bins)):\n",
        "      left_val = bins[left]\n",
        "      right_val = bins[right]\n",
        "      portion_arr = arrName[arrName[pivotColumn] >= left_val]\n",
        "      portion_arr = portion_arr[portion_arr[pivotColumn] < right_val]\n",
        "      if len(portion_arr) < 10 and right !=len(bins) - 1:\n",
        "        right = right + 1\n",
        "        continue\n",
        "      train_temp, test_temp = train_test_split(portion_arr, test_size = 0.2, random_state = seed)\n",
        "      #print(train_temp.values.shape)\n",
        "      #print(train.shape)\n",
        "      train = np.concatenate((train, np.array(train_temp.values)))\n",
        "      #val_temp, test_temp = train_test_split(val_test_temp, test_size = 0.5)\n",
        "      test_set = np.concatenate((test_set, np.array(test_temp.values)))\n",
        "\n",
        "      left = right\n",
        "      right = right + 1\n",
        "      portion_arr = []\n",
        "\n",
        "    return train, test_set"
      ],
      "metadata": {
        "id": "67hqY3DoGR1Y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_node_classification(features, z, features_test, labels_test, p_labeled=0.8, n_repeat=10, norm=False):\n",
        "    \"\"\"\n",
        "    Train a classifier using the node embeddings as features and reports the performance.\n",
        "    Parameters\n",
        "    ----------\n",
        "    features : array-like, shape [N, L]\n",
        "        The features used to train the classifier, i.e. the node embeddings\n",
        "    z : array-like, shape [N]\n",
        "        The ground truth labels\n",
        "    p_labeled : float\n",
        "        Percentage of nodes to use for training the classifier\n",
        "    n_repeat : int\n",
        "        Number of times to repeat the experiment\n",
        "    norm\n",
        "    Returns\n",
        "    -------\n",
        "    f1_micro: float\n",
        "        F_1 Score (micro) averaged of n_repeat trials.\n",
        "    f1_micro : float\n",
        "        F_1 Score (macro) averaged of n_repeat trials.\n",
        "    \"\"\"\n",
        "    '''if p_labeled == 0.8:\n",
        "        p_labeled = 1 - random.uniform(0.2, 0.8)'''\n",
        "\n",
        "    p_labeled = 0.5\n",
        "\n",
        "    if norm:\n",
        "        features = normalize(features)\n",
        "\n",
        "    trace = []\n",
        "    split_train1, split_train2 = None, None\n",
        "    for seed in range(n_repeat):\n",
        "        sss = ShuffleSplit(n_splits=1, test_size=1 - p_labeled, random_state=seed)\n",
        "        split_train, split_test = next(sss.split(features, z))\n",
        "\n",
        "        rfr = RandomForestRegressor(n_jobs=-1)\n",
        "        rfr.fit(features[split_train], z[split_train])\n",
        "        predicted = rfr.predict(features[split_test])\n",
        "\n",
        "        mae = mean_absolute_error(z[split_test], predicted)\n",
        "        mse = mean_squared_error(z[split_test], predicted)\n",
        "        \n",
        "        rfr = RandomForestRegressor(n_jobs=-1)\n",
        "        rfr.fit(features, z)\n",
        "        predicted = rfr.predict(features_test)\n",
        "\n",
        "        mae2 = mean_absolute_error(labels_test, predicted)\n",
        "        mse2 = mean_squared_error(labels_test, predicted)\n",
        "\n",
        "\n",
        "        trace.append((mae, mse, mae2, mse2))\n",
        "\n",
        "    return np.array(trace).mean(0)"
      ],
      "metadata": {
        "id": "zrhmpLT-GUIt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_performance(number_of_iter, test_indices):\n",
        "    ps = pd.read_pickle(embedding_path + 'gsne_cora_ml_embedding_graduate_second-order.pkl')\n",
        "    #pf = pd.read_csv('features_concatenated.csv', sep=',', header=None)\n",
        "    #raw_features = pf.values[14268:, 1:]\n",
        "    features_train = np.array([np.array(ps['mu'][k]) for k in range(14266, len(ps['mu'])) if k not in test_indices])\n",
        "    features_test = np.array([np.array(ps['mu'][k]) for k in range(14266, len(ps['mu'])) if k in test_indices])\n",
        "    labels_df = pd.read_csv('Property_price.csv')\n",
        "    labels_train = labels_df[~np.isin(labels_df['ID'], test_indices)]['price'].values\n",
        "    labels_test = labels_df[np.isin(labels_df['ID'], test_indices)]['price'].values\n",
        "    ''' for k in range(len(features)):\n",
        "      if np.random.rand() < 0.00:\n",
        "        print(str(k) + \" feats: \" +str(features[k]))'''\n",
        "\n",
        "    mae, mse, mae2, mse2 = score_node_classification(features_train, labels_train, features_test, labels_test, n_repeat = 1) \n",
        "    if number_of_iter % 300 == 0 or number_of_iter < 1000:\n",
        "      print(\"Embedding Features Results - MAE: \"+ str(mae)+\" RMSE: \"+ str(mse**0.5)+\" TMAE: \"+ str(mae2)+\" TRMSE: \"+ str(mse2**0.5))\n",
        "    return mse**0.5, mae2"
      ],
      "metadata": {
        "id": "ZsqBT0nVGVav"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('name', default='cora_ml')\n",
        "    parser.add_argument('model', default='gsne', help='gsne')\n",
        "    parser.add_argument('--suf', default='')\n",
        "    parser.add_argument('--proximity', default='second-order', help='first-order or second-order')\n",
        "    parser.add_argument('--embedding_dim', type=int, default=32)\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--K', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', default=0.0002)\n",
        "    parser.add_argument('--num_batches', type=int, default=600000)\n",
        "    parser.add_argument('--is_all', default=True)  # train with all edges; no validation or test set\n",
        "    args = parser.parse_args()\n",
        "    args.is_all = True if args.is_all == 'True' else False\n",
        "    train(args)"
      ],
      "metadata": {
        "id": "tFGW7YQ0GXRP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    \"\"\" graph_file = 'Data Tables Step 7 - Processed npzs_cleaned/graph_0_general_attributes.npz'\n",
        "    graph_file1 = 'Data Tables Step 7 - Processed npzs_cleaned/graph_1_house_region.npz'\n",
        "    graph_file2 = 'Data Tables Step 7 - Processed npzs_cleaned/graph_2_house_train.npz'\n",
        "    graph_file3 = 'Data Tables Step 7 - Processed npzs_cleaned/graph_3_property_school.npz'\n",
        "    graph_file4 = 'Data Tables Step 7 - Processed npzs_cleaned/graph_4_school_train.npz'\n",
        "    graph_file5 = 'Data Tables Step 7 - Processed npzs_cleaned/graph_5_train_train.npz' \"\"\"\n",
        "\n",
        "    graph_file = 'Data Tables Step 7 - Processed npzs/graph_0_general_attributes.npz'\n",
        "    graph_file1 = 'Data Tables Step 7 - Processed npzs/graph_1_house_region.npz'\n",
        "    graph_file2 = 'Data Tables Step 7 - Processed npzs/graph_2_house_train.npz'\n",
        "    graph_file3 = 'Data Tables Step 7 - Processed npzs/graph_3_property_school.npz'\n",
        "    graph_file4 = 'Data Tables Step 7 - Processed npzs/graph_4_school_train.npz'\n",
        "    graph_file5 = 'Data Tables Step 7 - Processed npzs/graph_5_train_train.npz' \n",
        "\n",
        "    #Dataset splitting for ensuring inductivity \n",
        "    price_file = 'Property_price.csv'\n",
        "    df_price = pd.read_csv(price_file)\n",
        "    train, tesst = train_test_split_sampled(df_price, 'price', 20)\n",
        "    train_indices = train[:, 0]\n",
        "    test_indices = tesst[:, 0]\n",
        "    np.savetxt(embedding_path + \"train.txt\", train_indices)\n",
        "    np.savetxt(embedding_path + \"test.txt\", test_indices)\n",
        "\n",
        "    #Normal attribute and graph loaders, added with test indices for ensuring they don't get trained\n",
        "    data_loader = DataUtils(graph_file, args.is_all, test_indices=test_indices) #THIS ONLY CONTAINS ATTRIBUTE INFO\n",
        "    data_loader1 = DataUtils(graph_file1, args.is_all, data_loader.node_negative_distribution_temp,test_indices=test_indices)\n",
        "    data_loader2 = DataUtils(graph_file2, args.is_all, data_loader.node_negative_distribution_temp,test_indices=test_indices)\n",
        "    data_loader3 = DataUtils(graph_file3, args.is_all, data_loader.node_negative_distribution_temp,test_indices=test_indices)\n",
        "    data_loader4 = DataUtils(graph_file4, args.is_all, data_loader.node_negative_distribution_temp,test_indices=test_indices)\n",
        "    data_loader5 = DataUtils(graph_file5, args.is_all, data_loader.node_negative_distribution_temp,test_indices=test_indices)\n",
        "\n",
        "    suffix = args.proximity\n",
        "    args.X1 = data_loader.X1 if args.suf != 'oh' else sp.identity(data_loader1.X1.shape[0])\n",
        "    args.X2 = data_loader.X2 if args.suf != 'oh' else sp.identity(data_loader2.X2.shape[0])\n",
        "    args.X3 = data_loader.X3 if args.suf != 'oh' else sp.identity(data_loader3.X3.shape[0])\n",
        "    args.X4 = data_loader.X4 if args.suf != 'oh' else sp.identity(data_loader4.X4.shape[0])\n",
        "\n",
        "    m = args.model\n",
        "    name = m + '_' + args.name\n",
        "    if 'gsne' == m:\n",
        "        model = GSNE(args)\n",
        "    else:\n",
        "        raise Exception(\"Only gsne available\")\n",
        "\n",
        "    writer = tf.summary.FileWriter(tensorboard_path)\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        #saver.restore(sess, model_path+\"model_graduate_best.ckpt\")\n",
        "        writer.add_graph(sess.graph)\n",
        "        print('-------------------------- ' + m + ' --------------------------')\n",
        "        print('batches\\tloss\\tsampling time\\ttraining_time\\tdatetime')\n",
        "\n",
        "        tf.global_variables_initializer().run()\n",
        "        sampling_time, training_time = 0, 0\n",
        "\n",
        "        previous_best = 9999999999\n",
        "\n",
        "        for b in range(args.num_batches):\n",
        "            if b%35000 == 0:\n",
        "              clear_output()\n",
        "            t1 = time.time()\n",
        "            #CREATE DIFFERENT SAMPLER\n",
        "            if b%5 < 0:\n",
        "                #sess.run(model.zero_ops)\n",
        "                u_i, u_j, label, node_type1, node_type2 = data_loader1.fetch_next_batch(batch_size=args.batch_size, K=args.K)\n",
        "            elif b%5 < 0:\n",
        "                u_i, u_j, label, node_type1, node_type2 = data_loader2.fetch_next_batch(batch_size=args.batch_size, K=args.K)\n",
        "            elif b%5 >= 0:\n",
        "                u_i, u_j, label, node_type1, node_type2 = data_loader3.fetch_next_batch(batch_size=args.batch_size, K=args.K)\n",
        "            elif b%5 == 3:\n",
        "                u_i, u_j, label, node_type1, node_type2 = data_loader4.fetch_next_batch(batch_size=args.batch_size, K=args.K)\n",
        "            else:\n",
        "                u_i, u_j, label, node_type1, node_type2 = data_loader5.fetch_next_batch(batch_size=args.batch_size, K=args.K)\n",
        "            \n",
        "            #u_i, u_j, label, w = data_loader.fetch_next_batch(batch_size=args.batch_size, K=args.K)\n",
        "            feed_dict = {model.u_i: u_i, model.u_j: u_j, model.label: label, model.node_type1 : node_type1, model.node_type2 : node_type2}\n",
        "\n",
        "            t2 = time.time()\n",
        "            sampling_time += t2 - t1\n",
        "\n",
        "            #loss, _ = sess.run([model.loss, model.accum_ops], feed_dict=feed_dict)\n",
        "            loss, _ = sess.run([model.loss, model.train_op], feed_dict=feed_dict)\n",
        "            ''' if (b%5 == 4 or True):\n",
        "              sess.run(model.train_step) '''\n",
        "            training_time += time.time() - t2\n",
        "\n",
        "            if b%5 < 5:\n",
        "              s = sess.run(model.merged_summary, feed_dict = feed_dict)\n",
        "              writer.add_summary(s, b)\n",
        "              writer.flush()\n",
        "\n",
        "\n",
        "            if b % 5000 < 5:\n",
        "                print('%d\\t%f\\t%0.2f\\t%0.2f\\t%s' % (b, loss, sampling_time, training_time,\n",
        "                                                        time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())))\n",
        "\n",
        "                sampling_time, training_time = 0, 0\n",
        "\n",
        "            if b!=0 and ( b % 50 == 0 or b == (args.num_batches - 1)):\n",
        "                if m == 'gsne':\n",
        "                    \n",
        "                    mu1, sigma1 = sess.run([model.embedding1, model.sigma1])\n",
        "                    mu2, sigma2 = sess.run([model.embedding2, model.sigma2])\n",
        "                    mu3, sigma3 = sess.run([model.embedding3, model.sigma3])\n",
        "                    mu4, sigma4 = sess.run([model.embedding4, model.sigma4])\n",
        "\n",
        "                    mu = copy.deepcopy(mu1)\n",
        "                    mu[node1_start: node1_end] = mu1[node1_start: node1_end]\n",
        "                    mu[node2_start: node2_end] = mu2[node2_start: node2_end]\n",
        "                    mu[node3_start: node3_end] = mu3[node3_start: node3_end]\n",
        "                    mu[node4_start: node4_end] = mu4[node4_start: node4_end]\n",
        "\n",
        "                    sigma = copy.deepcopy(sigma1)\n",
        "                    sigma[node1_start: node1_end] = sigma1[node1_start: node1_end]\n",
        "                    sigma[node2_start: node2_end] = sigma2[node2_start: node2_end]\n",
        "                    sigma[node3_start: node3_end] = sigma3[node3_start: node3_end]\n",
        "                    sigma[node4_start: node4_end] = sigma4[node4_start: node4_end]\n",
        "                    \n",
        "                    pickle.dump({'mu': data_loader.embedding_mapping(mu),\n",
        "                                 'sigma': data_loader.embedding_mapping(sigma)},\n",
        "                                open(embedding_path + '%s%s_embedding_graduate_%s.pkl' % (name, '_all' if args.is_all else '', suffix), 'wb'))\n",
        "                    \n",
        "                    save_path = saver.save(sess, model_path + \"model_graduate.ckpt\")\n",
        "                    \n",
        "                    curr_mae, val_mae = check_performance(number_of_iter = b, test_indices = test_indices)\n",
        "\n",
        "                    if curr_mae < previous_best:\n",
        "                      previous_best = curr_mae\n",
        "                      print(\"new best result train rmse: \"+str(curr_mae) + \" test mae:\"+str(val_mae))\n",
        "                      pickle.dump({'mu': data_loader.embedding_mapping(mu),\n",
        "                                 'sigma': data_loader.embedding_mapping(sigma)},\n",
        "                                open(embedding_path + '%s%s_embedding_graduate_%s_best.pkl' % (name, '_all' if args.is_all else '', suffix), 'wb'))\n",
        "                      save_path = saver.save(sess, model_path + \"model_graduate_best.ckpt\")\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\"only GSNE supported\")"
      ],
      "metadata": {
        "id": "3p5cB1r1Gb35"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EytBwnxbGdu_",
        "outputId": "dc28dc31-6ff3-4f12-8c97-d0fa8a513af9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n    main()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}